---
# title: "DADA2 Pipeline to analyse 16S microbial community data"
# project: "SNF project MiCo4Sys"
# institutions: "University of Bern, Institute for Fish and Wildlife Health (FIWI) and Institute for Ecology and Evolution"
# author: Jessica Rieder
#email: jessica.rieder@vetsuisse.unibe.ch
# corresponding author: Irene Adrian-Kalchhauser <irene.adrian-kalchhauser@vetsuisse.unibe.ch>

# last modified: 2022/04/01

# DADA2 Pipeline adopted from DADA2 Pipeline Tutorial (1.8)
# https://benjjneb.github.io/dada2/tutorial_1_8.html

# Input: demultiplexed fasta files
# Output: excel tables with ASVs and taxa

# The data processed included eDNA samples, water and biofilm, collected from two perch fish farms. 
# The fastq files were generated by amplicon sequencing (Illumina MiSeq, 2x300) of the 16s rRNA gene. 
# Primers: 341F/805R and 515F/806R
---

# Set the working directory
setwd("C:/Users/Jessica/Desktop/Combo")
getwd()

# Install Packages
BiocManager::install("dada2", version = "3.10")
install.packages("vegan", repos="https://cran.uib.no/", dependencies=T) 
install.packages("mice", repos="https://cran.uib.no/", dependencies=T) 
install.packages("dendextend", repos="https://cran.uib.no/", dependencies=T)

# Libraries
library("dada2") 
library("vegan") 
library("mice")
library("dendextend")
library("BiocManager")

# Set path

path = paste0(getwd())
list.files(path) 

# Unzip files before running DADA2

# Read in the names of the fastq files, perform string manipulation to get lists
# of the forward and reverse fastq files in match order:

# CONSIDERATIONS FOR DATA: The string manipulations may have to be modified
# if your filename format is different

# First need to remove the "xt48chgaj8a" part off the name.

fnFs = sort(list.files(path, pattern="_L1_R1_001.*", full.names = TRUE)) 

fnRs = sort(list.files(path, pattern="_L1_R2_001.*", full.names = TRUE)) 

sample.names = sapply(strsplit(basename(fnFs), "_"), `[`, 1) 

list(sample.names)


# Inspect Read Quality Profiles

# In gray-scale is a heat map of the frequency of each quality score at each 
# base position. The mean quality score at each position is shown by the green 
# line, and the quartiles of the quality score distribution by the orange lines. 
# The red line shows the scaled proportion of reads that extend to at least that 
# position (this is more useful for other sequencing technologies, as Illumina 
# reads are typically all the same length, hence the flat red line).

pdf("Quality_Profiles_Combo.pdf", width=15, height=10) 
plotQualityProfile(fnFs[1:40]) 
plotQualityProfile(fnRs[1:40]) 
dev.off()

# Filter and Trim
# The reverse reads are of significantly worse quality, especially at the end, 
# which is common in Illumina sequencing. This isn?t too worrisome, 
# as DADA2 incorporates quality information into its error model which makes 
# the algorithm robust to lower quality sequence, but trimming as the average 
# qualities crash will improve the algorithm?s sensitivity to rare sequence 
# variants. 

# CONSIDERATIONS for your own data: Your reads must still overlap after 
# truncation in order to merge them later! For example, is using 2x250 V4 
# sequence data, the forward and reverse reads almost completely overlap and 
# trimming can be completely guided by the quality scores. If you are using
# a less-overlapping primer set, like V1-V2 or V3-V4, your truncLen must be 
# large enough to maintain 20 + biological.length.variation nucleotides of 
# overlap between them.


#Place filtered files in filtered/ subdirectory 

filtFs = file.path(path = path, "Filtered", 
paste0(sample.names, "_F_filt.fastq")) 

filtRs = file.path(path = path, "Filtered", 
paste0(sample.names, "_R_filt.fastq")) 

# This next step takes some time! It cannot be multithreaded on Windows. 
# Standard: maxN = 0 (DADA2 requires no Ns), truncQ=2, rm.phix=TRUE, maxEE=2
# maxEE parameter sets the maximum number of "expected errors" allowed in a read
# which is a better filter than simply averaging quality scores 
# If too few reads are passing the filter, consider relaxing maxEE, perhaps 
# especially on the reverse reads (eg. maxEE=c(2,5)), and reducing the truncLen 
# to remove low quality tails. Remember though, when choosing truncLen for 
# paired-end reads you must maintain overlap after truncation in order to 
# merge them later.

#On Windows set multithread=FALSE; on Linux environments, set multithtread=TRUE. 

out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
trimLeft = c(19,21), 
trimRight = c(10,90),
maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=FALSE) 
out

# Error Rates and Plots
# The DADA2 algorithm makes use of a parametric error model (err) and every 
# amplicon dataset has a different set of error rates. The learnErrors method 
# learns this error model from the data, by alternating estimation of the error 
# rates and inference of sample composition until they converge on a jointly 
# consistent solution. As in many machine-learning problems, the algorithm must 
# begin with an initial guess, for which the maximum possible error rates in 
# this data are used (the error rates if only the most abundant sequence is 
# correct and all the rest are errors).

filtFs = file.path(path = path, "Filtered", paste0(sample.names, 
"_F_filt.fastq")) 

filtRs = file.path(path = path, "Filtered", paste0(sample.names, 
"_R_filt.fastq"))

errF = learnErrors(filtFs, multithread=TRUE) 
errR = learnErrors(filtRs, multithread=TRUE) 

# Sanity Check!
# The red line shows the error rates expected under the nominal definition of 
# the Q-score. Here the estimated error rates (black line) are a good fit to the 
# observed rates (points), and the error rates drop with increased quality as 
# expected.

pdf("Error_Plots_Combo.pdf") 
plotErrors(errF, nominalQ=TRUE) 
plotErrors(errR, nominalQ=TRUE) 
dev.off() 

# Dereplication and Sample Inference 
# Dereplication combines all identical sequencing reads into into 
# ?unique sequences? with a corresponding ?abundance? equal to the number of 
# reads with that unique sequence. Dereplication substantially reduces 
# computation time by eliminating redundant comparisons.

# Dereplication in the DADA2 pipeline has one crucial addition from other 
# pipelines: DADA2 retains a summary of the quality information #associated with 
# each unique sequence. The consensus quality profile of a unique sequence is 
# the average of the positional qualities from the dereplicated reads. 
# These quality profiles inform the error model of the subsequent sample 
# inference step, significantly increasing DADA2?s accuracy.

derepFs = derepFastq(filtFs, verbose=TRUE) 
derepRs = derepFastq(filtRs, verbose=TRUE) 
names(derepFs) = sample.names 
names(derepRs) = sample.names

dadaFs = dada(derepFs, err=errF, multithread=TRUE) 
dadaRs = dada(derepRs, err=errR, multithread=TRUE) 
head(dadaFs) [[1]]


# Merge Paired Ends
# Merge the forward and reverse reads together to obtain the full denoised 
# sequences. Merging is performed by aligning the denoised forward reads with 
# the reverse-complement of the corresponding denoised reverse reads, and then
# constructing the merged ?contig? sequences. By default, merged sequences are 
# only output if the forward and reverse reads overlap by at least 12 bases, 
# and are identical to each other in the overlap region.

# CONSIDERATIONS for your own data: Most of your reads should successfully 
# merge. If that is not the case upstream parameters may need to be revisited: 
# Did you trim away the overlap between your reads?

# Extensions: Non-overlapping reads are supported, but not recommended, with
# mergePairs(..., justConcatenate=TRUE)

mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE) 
head(mergers[[1]])

# Construct Amplicon Sequence Variant (ASV) Table 
# A higher resolution version of the OTU table produced by traditional methods. 
# The sequence table is a matrix with rows corresponding to (and named by) the 
# samples, and columns corresponding to (and named by) the sequence variants.

# CONSIDERATIONS for data: Sequences that are much longer or shorter than 
# expected may be the result of non-specific priming. You can remove 
# non-target-length sequences with base R manipulations of the sequence 
# table (eg. seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(250,256)]). 
# This is analogous to ?cutting a band? in-silico to get amplicons of the 
# targeted length.

seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# 40 23046

# inspect distribution of sequence lengths

table(nchar(getSequences(seqtab))) 

write.csv(seqtab, file ="ASV_Table_Mi_Earth") 

# Remove Chimeras
# The core dada method corrects substitution and indel errors, but chimeras 
# remain. Fortunately, the accuracy of the sequence variants after denoising 
# makes identifying chimeras simpler than it is when dealing with fuzzy OTUs. 
# Chimeric sequences are identified if they can be exactly reconstructed by 
# combining a left-segment and a right-segment from two more abundant ?parent? 
# sequences.

# The frequency of chimeric sequences varies substantially from dataset to 
# dataset, and depends on on factors including experimental procedures and 
#sample complexity. 

# CONSIDERATIONS for data: Most of your reads should remain after chimera 
# removal (it is not uncommon for a majority of sequence variants to be removed 
# though). If most of your reads were removed as chimeric, upstream processing 
# may need to be revisited. In almost all cases this is caused by primer 
# sequences with ambiguous nucleotides that were not removed prior to 
# beginning the DADA2 pipeline


seqtab.nochim <- removeBimeraDenovo(seqtab, minFoldParentOverAbundance=8, 
			method="consensus", multi=TRUE)
write.table(t(seqtab.nochim), "Seqtab_NoChim_Table_Mi_Earth", sep = "\t", quote = FALSE)

dim(seqtab.nochim)
# 40 18072
sum(seqtab.nochim)/sum(seqtab)
# 0.9882939

# Track Reads Through the Pipeline 
# A final check of the progress, look at the number of reads that made it 
# through each step of the pipeline.

# CONSIDERATIONS for data: Outside of filtering (depending on how stringent 
# you want to be) there should no step in which a majority of reads are lost. 
# If a majority of reads failed to merge, you may need to revisit the truncLen 
# parameter used in the filtering step and make sure that the truncated reads 
# span your amplicon. If a majority of reads were removed as chimeric, you may 
# need to revisit the removal of primers, as the ambiguous nucleotides in 
# unremoved primers interfere with chimera identification.

getN = function(x) sum(getUniques(x)) 

track = cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), 
sapply(mergers, getN), rowSums(seqtab.nochim)) 

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", 
"merged", "nonchim")

rownames(track) <- sample.names 

track 

write.table(track, "Track_Mi_Earth.txt", quote =FALSE, sep="\t", row.names=TRUE)
 
# Assign Taxonomy
# CONSIDERATIONS for data: If your reads do not seem to be appropriately 
# assigned, for example lots of your bacterial 16S sequences are being 
# assigned as Eukaryota NA NA NA NA NA, your reads may be in the opposite 
# orientation as the reference database. Tell dada2 to try the 
# reverse-complement orientation with assignTaxonomy(..., tryRC=TRUE) 
# and see if this fixes the assignments

# Naive Baysian with a bootstrap of 100 with minboot = 50
taxa = assignTaxonomy(seqtab.nochim, 
paste0(path, "/silva_nr99_v138_train_set.fa.gz"), multithread=TRUE, 
tryRC = TRUE, outputBootstraps = TRUE)
write.table(t(taxa), "Taxa_Bootstrap_MiEarth.txt", quote = FALSE, sep="\t", row.names=TRUE)

# Removing sequence rownames for display only
taxa.print = taxa  

rownames(taxa.print) = NULL 

write.table(taxa.print, "Taxa_Mi_Earth.txt", quote =FALSE, sep="\t", row.names=TRUE)

# See the first rows of the taxonomy object
head(taxa.print)  

# Write Sequence Table to File as Standard OTU Table 
OTU.print = seqtab.nochim
colnames(OTU.print) = seq(1, ncol(OTU.print), 1) 
write.table(t(OTU.print), file="ASV_Mi_Earth.txt", quote =FALSE, sep="\t", row.names=TRUE)


